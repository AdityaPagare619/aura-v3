# **Autonomous Offline AI Agents on Resource-Constrained Edge Hardware: Strategies, Architectures, and Models for 2026**

The transition from cloud-tethered artificial intelligence applications to fully autonomous, offline agents operating natively on mobile hardware constitutes one of the most rigorous engineering challenges of 2026\. Historically, agentic frameworks engineered to orchestrate multi-step workflows, manage file systems, and execute complex device commands have relied exclusively on proprietary, cloud-hosted frontier models. Systems powered by models such as Claude Opus 4.6 or Gemini 3.1 Pro achieve near-human efficacy in task planning and application control precisely because they leverage immense parameter counts, sustained external memory arrays, and virtually unlimited datacenter compute.1 However, replicating this fluid, autonomous behavior on an offline mobile device subject to strict 8GB or 16GB memory constraints—without utilizing external application programming interface (API) keys—requires fundamentally rethinking the foundational architecture of the agent.4

Early attempts to deploy open-weight small language models (SLMs) such as the initial iterations of Phi-3-mini (3.8B), Mistral-7B, and Llama-3 (8B) as central reasoning engines for mobile assistants frequently resulted in cascading failures. These models, while highly competent at single-turn text summarization or basic conversational outputs, lacked the requisite cognitive depth for long-horizon task planning. When detached from robust server-side scaffolding, these early SLMs often fell into recursive execution loops, suffered from severe context degradation, or hallucinated tool calls when tasked with operating mobile interfaces autonomously.7 The design principles and privacy mandates that necessitate an entirely offline environment inherently deprive the system of the computational leniency afforded by company servers.

As of 2026, the industry consensus has recognized that bridging the performance gap between a sub-10-billion-parameter edge model and a frontier model like Claude Opus 4.6 cannot be achieved through traditional parameter scaling or monolithic generation.9 Instead, the solution demands a comprehensive integration of highly specialized inference-time reasoning algorithms, extreme low-bit quantization, state-space hybrid architectures, and novel pre-computation frameworks for graphical user interface (GUI) automation.5 This report details the specific hardware constraints governing mobile edge inference, evaluates the state-of-the-art SLMs engineered for local reasoning, and proposes a highly modular, multi-agent architecture capable of delivering elite autonomous capabilities entirely offline.

## **The Physical Boundaries of Mobile Intelligence**

To construct a highly capable offline assistant, it is imperative to first isolate and quantify the physical boundaries of mobile compute. The primary constraints dictating agentic performance on edge devices are memory capacity, memory bandwidth, and thermal dissipation limits inherent to the smartphone form factor.5 Understanding these variables is critical for selecting the appropriate quantization techniques and execution frameworks.

### **Memory Capacity and the Quantization Imperative**

While modern flagship smartphones frequently advertise 8GB to 16GB of unified Random Access Memory (RAM), this entire pool cannot be unilaterally allocated to an artificial intelligence workload. The host operating system, essential background services, and the target applications the agent must manipulate concurrently consume a baseline of 4GB to 6GB of RAM.5 Consequently, the true memory budget available for an onboard language model and its associated orchestration framework is generally restricted to a narrow window of 4GB to 10GB.12

To determine the exact memory footprint of a local model, the following calculation serves as the standard metric: ![][image1] Where ![][image2].12

Under these mathematics, an 8-billion-parameter model operating in standard 16-bit floating-point (FP16) precision requires approximately 16GB of RAM merely to house the static weights, immediately triggering an out-of-memory (OOM) termination on the vast majority of mobile hardware.12 Therefore, sub-byte quantization is strictly mandatory. Advanced quantization techniques, such as 4-bit variants (e.g., GGUF or AWQ formats), compress an 8B model to an operational footprint of approximately 4GB to 5GB, allowing the weights to reside safely within mobile memory constraints.5

However, the context window scaling introduces a secondary, highly volatile memory tax. The Key-Value (KV) cache, which stores the intermediate attention states of past tokens to accelerate autoregressive decoding, scales linearly with the context length, the batch size, and the internal hidden dimensions of the model.15 For an autonomous agent to orchestrate multi-step workflows, it must continuously ingest a substantial prompt containing previous actions, system states, OmniParser outputs, and application rules. A 32,000-token context window can easily consume an additional 800MB to 1.5GB of RAM.12 When compounded with the quantized weights, the total memory envelope pushes a 7B model perilously close to the operating system's termination threshold, necessitating highly optimized memory eviction protocols and sparse attention mechanisms.5

### **The Memory Bandwidth Bottleneck**

Contrary to prevailing assumptions, mobile devices do not fundamentally lack computational logic capacity; rather, they are severely bottlenecked by memory bandwidth limits. During the autoregressive decoding phase—where the model generates reasoning traces or tool calls one token at a time—the inference engine must stream the entire active weight matrix from the RAM to the Neural Processing Unit (NPU) or Central Processing Unit (CPU) for every single token generated.5

Enterprise data center graphics processing units (GPUs) operate with memory bandwidths ranging from 2 to 3 terabytes per second. In stark contrast, advanced mobile systems-on-chip (SoCs), such as the Qualcomm Snapdragon 8 Gen 5, typically provide a bandwidth of only 50 to 90 gigabytes per second.5 This 30x to 50x structural disparity dictates that decode-time inference on mobile devices is strictly memory-bound. Even if the device's NPU possesses immense raw computational power—such as the Hexagon NPU's substantial tera operations per second (TOPS)—the processor compute units will remain idle while waiting for the weights to traverse the narrow memory bus.5 Consequently, reducing the data precision from 16-bit to 4-bit not only halves the storage requirement but fundamentally accelerates the generation speed by reducing the memory traffic per token by a proportional factor.5

## **The 1-Bit Paradigm: Overcoming the Bandwidth Wall**

To categorically bypass traditional bandwidth constraints and thermal limits, the 1-bit LLM architecture has emerged as a revolutionary solution for mobile agent deployments in 2026\. Microsoft's native 1-bit Large Language Model, structured under the BitNet b1.58 framework, represents a fundamental restructuring of neural network arithmetic.19

Instead of utilizing continuous FP16 or 8-bit integer weights, the BitNet b1.58 architecture constrains the model weights to discrete ternary values: ![][image3].21 Critically, this drastic quantization occurs from scratch during the pre-training phase, avoiding the severe accuracy degradation and perplexity spikes typically associated with post-training quantization methods applied to traditional dense models.19

The architectural implications for offline mobile agents are profoundly advantageous:

| Metric | Traditional FP16 Model (2.4B) | BitNet b1.58 Model (2.4B) | Impact on Mobile Deployment |
| :---- | :---- | :---- | :---- |
| **Weight Memory Footprint** | \~4.8 GB | \~1.3 GB | Allows concurrent execution of auxiliary models (e.g., Vision Parsers) within an 8GB budget. |
| **Core Compute Operation** | Floating-Point Multiplication | Integer Addition/Subtraction | Eradicates the most energy-intensive mathematical operations in the transformer network. |
| **Energy Consumption** | Baseline | \-55.4% to \-70.0% | Prevents rapid battery drain and mitigates thermal throttling during sustained agentic loops. |
| **Decoding Speed (CPU)** | 10 \- 15 tokens/sec | 36.9 tokens/sec | Achieves fluent, human-readable generation speeds without requiring NPU offloading. |

Table 1: Comparative analysis of FP16 versus BitNet b1.58 architectures for edge inference.20

By restricting the weights to ternary values, BitNet converts complex matrix multiplications into highly efficient, hardware-friendly scalar additions.21 The operational footprint is so minimal that a 2.4-billion-parameter model, such as the BitNet b1.58 2B4T variant, consumes roughly 1.3GB of RAM.23 This extreme compression preserves enough system memory to maintain complex context windows and run parallel background tasks without inducing OS-level instability.

Deployment of these ternary models on mobile operating systems is facilitated through the highly optimized bitnet.cpp inference framework. Developers integrating this framework into custom offline agent applications utilize the Android Native Development Kit (NDK) via C++, bypassing the sluggish Java Virtual Machine (JVM) layer entirely to execute computationally intensive AI workloads directly on the device's bare metal.20 This level of hardware optimization ensures that the agent remains responsive even when detached from any external API services.

## **Architecting the SLM Reasoning Core**

While hardware optimizations and 1-bit topologies enable the raw execution of language models on mobile devices, the overall efficacy of the agent relies entirely on the cognitive architecture of the underlying SLM. A central planner intended to replace robust, cloud-based tools must exhibit the capacity to execute advanced reasoning, construct flawless JSON-formatted tool calls, parse complex screen hierarchies, and adapt gracefully to execution errors. To approach the baseline established by Claude Opus 4.6 or Gemini 3.1 Pro, the industry has shifted away from monolithic parameter scaling toward highly specialized inference-time compute protocols.2

In 2026, the most effective strategy involves utilizing SLMs explicitly trained to generate internal reasoning traces—often referred to as "thinking models"—prior to outputting an actionable command.11 By intentionally allocating additional computational cycles during the decoding phase to deliberate, verify logic, and formulate a plan, these compact models transcend their parametric limitations.

### **LFM2.5-1.2B-Thinking: The Ultra-Lightweight Router**

The LFM2.5-1.2B-Thinking model from Liquid AI exemplifies the pinnacle of extreme edge efficiency coupled with sophisticated logic. Despite possessing a mere 1.2 billion parameters, this model leverages a highly optimized hybrid architecture combining traditional transformer attention mechanisms with state-space convolutional layers.27 This structural fusion enables the model to operate within a remarkably small footprint of less than 900MB of RAM, while processing at a decoding speed of 82 tokens per second on a modern mobile NPU, such as the Snapdragon 8 Elite Gen 5\.27

Crucially, the LFM2.5-1.2B-Thinking variant is explicitly trained to execute a "reason first, then answer" protocol.27 When an offline mobile agent utilizes this model to parse a complex user request, the model internally breaks the problem into sub-tasks, evaluates the required tool endpoints, verifies intermediate logical steps, and sequences the final execution path.30 On complex reasoning benchmarks, the presence of these internal thinking traces allows this 1.2B model to rival or exceed the performance of models three times its size.28 Within an offline mobile architecture, this model serves flawlessly as the "Central Router"—a highly resilient, always-on planner that continuously manages the execution loop without monopolizing system resources or draining the battery.30

### **Phi-4-mini-flash-reasoning: Advanced Algorithmic Logic**

For agentic workflows requiring rigorous algorithmic problem-solving or complex data transformation, the Microsoft Phi-4-mini-flash-reasoning model (3.8 billion parameters) serves as a potent core engine.31 Built upon the novel SambaY decoder-hybrid-decoder architecture, the model merges the linear scaling efficiency of Mamba (a State Space Model) with a selective sliding window attention mechanism.31 This hybridization achieves up to a tenfold increase in throughput and a drastic reduction in latency compared to traditional dense transformers, while supporting an expansive 64K token context length.31

An extensive context window is absolutely vital for mobile agents that must process extensive system logs, deeply nested JSON API responses from local applications, or long, sustained conversation histories.31 The Phi-4 reasoning variants consistently outperform significantly larger models on instruction following, deterministic mathematics, and tool-calling benchmarks because their pre-training and post-training data pipelines were intensely curated for pure logic and chain-of-thought fidelity rather than raw factual recall.33

### **DeepSeek-R1-Distill-Qwen-7B: High-Density Expert Swapping**

When the mobile agent encounters a workflow that completely exceeds the logical capacity of a 1B or 3B model—such as writing and debugging a complex local Python script, or parsing an intricate offline database—the system requires an expert-level intervention. The DeepSeek-R1-Distill-Qwen-7B model represents the apex of 7B reasoning in the 2026 landscape.35

Distilled directly from the massive 671-billion-parameter DeepSeek-R1 utilizing 800,000 highly curated reasoning samples, this dense transformer model achieves an exceptional 92.8% accuracy on the MATH-500 benchmark and a formidable pass rate on the challenging AIME 2024 evaluations.35 When quantized to 4-bit, it fits within a 4GB to 6GB memory footprint.5 While it is too resource-intensive to run continuously as the primary agent loop on an 8GB device, it serves as the ultimate "swappable expert." The lightweight central router can dynamically page the DeepSeek distilled 7B model into active memory to resolve a specific, dense logical blockade, ensuring that the offline assistant maintains reasoning capabilities that rival the outputs of early proprietary models like GPT-4, entirely detached from the cloud.35

## **Visual Grounding and Spatial Semantics**

A fundamental limitation of previous mobile agent frameworks was their strict reliance on the Android Accessibility tree or iOS View Hierarchies to understand the state of the device. These structural representations are frequently inaccurate, intentionally obfuscated by application developers for security reasons, or entirely absent in dynamically rendered frameworks like Flutter, React Native, or mobile games.37 For an offline agent to possess genuine autonomy across all applications, it must interact with the device visually, interpreting the screen exactly as a human user does.

### **Multimodal SLMs: Qwen3.5-VL and Gemma 3**

To achieve visual grounding, one approach is the deployment of native multimodal SLMs. The Qwen3.5-VL-7B-Instruct model is engineered specifically for dynamic resolution visual comprehension.39 It inherently understands spatial layouts, charts, and complex application interfaces, allowing the agent to map natural language commands to exact pixel coordinates on the screen.39

Similarly, Google's Gemma 3 (available in 1B, 4B, and 12B configurations) provides exceptional multimodal capabilities paired with an expansive 128K context window.41 The 4B variant strikes an optimal balance for mobile execution, permitting the agent to process multiple continuous screenshots over long-horizon tasks while supporting global localization across 140 languages.41

### **OmniParser V2: Semantic Vision for Dynamic States**

However, passing high-resolution image tensors through a 7B multimodal model at every step of a workflow incurs massive latency and memory overhead, severely degrading the user experience. A superior architectural approach separates the perception mechanism from the reasoning mechanism through the implementation of an intermediate semantic parsing layer. Microsoft's OmniParser V2 provides this exact capability.44

Designed as an ultra-fast screen tokenizer, OmniParser V2 intercepts the raw pixel data of the mobile screen and converts it into a highly structured textual representation.44 It utilizes a dual-module system:

1. **Detection Module:** A highly fine-tuned YOLOv8 object detection model instantly scans the screenshot to identify all interactive regions—such as buttons, toggles, text fields, and icons. It draws precise bounding boxes around these elements and assigns them unique numerical identifiers.37  
2. **Captioning Module:** A lightweight foundational vision model (Florence-2) processes the cropped regions, extracting embedded text and inferring the functional semantics of abstract icons (e.g., recognizing a gear symbol and generating the label "Settings configuration").46

The resulting output is a clean, structured JSON or Markdown representation of the screen's interactive surface. By passing this text-based representation to the SLM router (such as LFM2.5-1.2B) rather than the raw image, the system circumvents the computational bottleneck of vision encoders.44 The reasoning model simply outputs a command such as {"action": "click", "target\_id": 14}, which the low-level orchestration layer maps back to the precise pixel coordinates provided by OmniParser to execute a simulated touch event.37 Crucially, the V2 iteration reduces parsing latency by 60% compared to its predecessor, processing a full frame in fractions of a second on NPU-accelerated hardware, making it viable for rapid mobile execution.46

## **Eradicating Latency with Topological Mapping**

While OmniParser flawlessly translates dynamic and unknown interfaces into actionable text, relying on a continuous "perception-reason-action" loop for every single interaction introduces unacceptable cumulative latency. An autonomous task requiring ten distinct UI interactions would require ten separate invocations of the LLM, resulting in a sluggish experience that tests the patience of the user.10 To resolve this and achieve the seamless workflow orchestration demanded by the user, the state-of-the-art framework for 2026 relies on the offline construction of application topologies, a methodology pioneered by GraphPilot.48

GraphPilot fundamentally alters the agentic paradigm by shifting the computational burden from online generation to offline pre-computation. The architecture eliminates the uncertainty of UI navigation by supplying the reasoning model with an explicit map of the application environment.48

### **The Knowledge Graph Architecture**

The GraphPilot system operates in two distinct phases:

1. **Offline Phase (Graph Construction):** When the mobile device is idle, charging, or disconnected, the agent autonomously explores frequently used local applications. It systematically records state changes, visual elements, and UI hierarchies to construct an app-specific Knowledge Graph.10 Within this highly structured bipartite graph ![][image4], the nodes (![][image5]) represent individual application pages and interactable elements, while the edges (![][image6]) represent the transition rules—the specific actions required to navigate from one node to the next.10  
2. **Online Phase (One-Step Planning):** When a user issues a complex command (e.g., "Extract the tracking number from my latest email and check its status in the delivery app"), GraphPilot does not query the LLM step-by-step. Instead, it retrieves the relevant Knowledge Graphs for the required applications and injects this topological data directly into the SLM's prompt context.48

Because the model now possesses a complete, infallible map of the application's internal logic, it does not need to guess the outcome of its actions. The "Thinking" SLM logically deduces the shortest path to the objective and generates the *entire* multi-step sequence of actions in a single inference pass.10 A validator module then rapidly checks the generated sequence against the transition rules in the knowledge graph, performing iterative corrections to ensure execution safety.48

This one-step reasoning capability eliminates the repetitive query-act loop. Empirical benchmarks on the rigorous DroidTask suite demonstrate that GraphPilot reduces the number of required LLM invocations by over 77.3% compared to traditional stepwise agents like AutoDroid and Mind2Web, slashing overall task latency by approximately 70%.10 If the agent encounters an unmapped state or an unexpected system pop-up during execution, it dynamically falls back to OmniParser V2, re-orienting itself visually before updating the Knowledge Graph and continuing the task.44 This synthesis of offline graph retrieval and online visual grounding forms the bedrock of a highly capable, API-free mobile agent.

## **Hardware Acceleration and Alignment for the Edge**

Generating reasoning traces and comprehensive action sequences on mobile hardware requires profound optimization to maintain fluidity. To ensure the SLM operates at peak efficiency, the underlying execution engine must aggressively leverage the Neural Processing Unit.

### **NPU-Coordinated Speculative Decoding**

Standard autoregressive generation relies heavily on matrix-vector multiplications, an operation format that severely underutilizes the matrix-matrix optimized tensor cores present in modern NPUs like the Snapdragon 8 Elite Gen 5\.49 To rectify this hardware underutilization, the implementation of NPU-centric Speculative Decoding is required.26

Speculative decoding mitigates the sequential bottleneck by utilizing an ultra-fast, parameter-efficient draft mechanism to predict a sequence of several upcoming tokens simultaneously.26 The larger target model (e.g., the 3.8B or 7B SLM) then processes this entire drafted sequence in a single forward pass, validating the correct tokens in parallel.26 Because the verification pass is a dense matrix-matrix operation, it saturates the NPU's compute units with maximum efficiency.

Furthermore, within an agentic framework utilizing GraphPilot, the system can leverage Retrieval-Based Speculative Decoding (often categorized under Context-Augmented Generation). In this schema, the draft mechanism pulls candidate tokens directly from the injected Knowledge Graph, bypassing the need for a separate draft model entirely.51 This specialized framework, referred to in current literature as sd.npu, translates idle NPU cycles into valid token throughput, achieving inference speedups of up to 3.81x and yielding massive energy savings on edge devices, thereby masking the inherent memory bandwidth limitations.49

### **ExPO: Alignment via Self-Explanation**

Finally, aligning these compact models to execute GraphPilot queries and OmniParser schemas flawlessly requires targeted reinforcement learning. Historically, fine-tuning SLMs for advanced reasoning required distilling trajectories from massive proprietary models. However, an emerging technique known as Self-Explanation Policy Optimization (ExPO) provides a highly efficient, entirely localized alternative.53

Instead of fine-tuning the SLM on reasoning traces generated by models like Claude Opus—which may utilize vocabulary matrices and conceptual leaps that are fundamentally out-of-distribution for a 3B model—ExPO forces the SLM to generate its own reasoning paths.53 During the offline tuning phase, the system provides the model with a complex task and the known ground-truth answer. The model is forced to explore the latent space to construct a logical self-explanation connecting the initial state to the correct outcome.55

Because these self-generated explanations inherently respect the model's parametric constraints, they provide a vastly superior learning signal.53 Applying reinforcement learning algorithms, such as Direct Preference Optimization (DPO), over these self-generated traces dramatically improves the model's ability to navigate complex reasoning trees on difficult tasks.53 For an offline agent, ExPO ensures that the model learns to format tools, balance workflows, and self-correct errors using logic entirely native to its own size, maximizing operational reliability without increasing the parameter count.

## **The Agentic Router Architecture: A Synthesis**

To deliver an offline, API-free assistant that rivals the reasoning levels of Claude Opus 4.6 while strictly adhering to an 8GB or 16GB mobile memory limit, developers must abandon the monolithic LLM design. The optimal solution is a highly modular, heterogeneous system that leverages the specific strengths of the aforementioned technologies in a coordinated pipeline.

The recommended architectural blueprint for a state-of-the-art 2026 mobile agent is structured as follows:

1. **The Low-Level Execution Layer:** The foundation of the application must bypass interpreted languages (like Python) in production to eliminate overhead. The execution layer should utilize **ExecuTorch** or the **bitnet.cpp** framework, deployed via the Android NDK or iOS Objective-C++.24 This ensures that the NPU is directly targeted, memory allocation is statically planned ahead of time, and the continuous execution flow remains uninhibited.49  
2. **The Structural Memory Layer:** An embedded, locally hosted database (such as a lightweight instance of SQLite or a localized vector store like Chroma) manages the **GraphPilot** application topologies and user persona data.10 This allows the agent to recall past interactions and inject mapped application states into the context window instantly.  
3. **The Central Router (The "Brain"):** The core orchestration node, constantly active and resident in memory, is an ultra-compact reasoning model. The **LFM2.5-1.2B-Thinking** model serves this role.27 Consuming minimal RAM and operating at high speeds, it analyzes user intent, queries the GraphPilot database, and generates rapid reasoning traces to formulate action plans.27  
4. **The Perception Fallback:** If the user navigates to an unmapped screen, **OmniParser V2** is triggered asynchronously. Operating efficiently on the NPU, it instantly tokenizes the unknown GUI into structured textual elements, feeding the spatial data back to the router without requiring a massive multimodal payload.44  
5. **The Deep Reasoning Swap:** When the router encounters a workflow exceeding its logical capacity (e.g., executing complex mathematical operations or parsing massive data sets), the system implements an agentic handoff. The router temporarily suspends, and a heavily quantized, high-density reasoning specialist—such as **Phi-4-mini-flash-reasoning** (3.8B) or **DeepSeek-R1-Distill-Qwen** (7B)—is paged into active memory. Accelerated by NPU Speculative Decoding, the specialist resolves the bottleneck, returns the solution, and is subsequently flushed from memory to preserve system stability.31

## **Comparative Benchmarks and Frontier Capabilities**

To accurately assess the efficacy of this offline architecture, the performance metrics must be evaluated against the standards set by proprietary models. On the GDPval-AA benchmark, an evaluation of performance on economically valuable, multi-step knowledge work tasks, models like Claude Opus 4.6 previously defined the upper echelon of capability.3 While an isolated 3-billion parameter model cannot equal Opus 4.6 on zero-shot, open-domain encyclopedic queries, the orchestrated mobile system described above achieves highly competitive task completion rates on bounded, device-specific workflows.10

The Tau2-bench (Telecom) evaluation, which measures an agent's ability to coordinate tool use and interact with simulated environments to resolve complex issues, highlights the profound shift toward inference-time specialization.1 Specialized SLMs consistently demonstrate that when their generation patterns are aligned to tool-calling syntax via ExPO reinforcement learning, they can navigate long-horizon logic trees with extreme precision.1 The performance of distilled models on benchmarks such as AIME 2024 and LiveCodeBench confirms that state-of-the-art mathematical and procedural reasoning is fully achievable in accessible, economically deployable SLMs operating within strict hardware envelopes.35

## **Conclusion**

Constructing a sophisticated, fully offline artificial intelligence assistant for mobile devices in 2026 is an entirely viable engineering endeavor that represents the bleeding edge of agentic design. The inherent hardware constraints of 8GB to 16GB of memory, once considered an insurmountable barrier to complex reasoning and task planning, have been successfully circumvented through a convergence of specialized SLMs, radical 1-bit quantization architectures, and intelligent framework orchestration.

By leveraging highly optimized "Thinking" models such as LFM2.5-1.2B and Phi-4-mini-reasoning, developers can establish a rapid, logical core that operates continuously within a minimal memory footprint. Coupling these advanced reasoning models with the GraphPilot methodology fundamentally solves the latency crisis associated with step-by-step GUI automation by allowing the model to map complex UI trajectories in a single inference pass. When augmented with OmniParser V2 for dynamic visual semantics and NPU-coordinated speculative decoding for raw execution speed, the resulting system achieves a level of autonomous operational fluidity that rivals cloud-based frontier models like Claude Opus 4.6, entirely offline and completely free of API dependencies.

The future of mobile agentic AI does not lie in the futile attempt to compress monolithic, general-purpose models into edge devices, but rather in orchestrating a synchronous pipeline of highly specialized, context-aware, and inference-optimized components executing directly on the silicon.

#### **Works cited**

1. The best AI models in 2026: What model to pick for your use case \- Pluralsight, accessed on February 22, 2026, [https://www.pluralsight.com/resources/blog/ai-and-data/best-ai-models-2026-list](https://www.pluralsight.com/resources/blog/ai-and-data/best-ai-models-2026-list)  
2. Models \- OpenRouter, accessed on February 22, 2026, [https://openrouter.ai/models](https://openrouter.ai/models)  
3. Introducing Claude Opus 4.6, accessed on February 22, 2026, [https://www.anthropic.com/news/claude-opus-4-6](https://www.anthropic.com/news/claude-opus-4-6)  
4. 10 Best AI Tools That Work Offline in 2026 | ClickUp, accessed on February 22, 2026, [https://clickup.com/blog/offline-ai-tools/](https://clickup.com/blog/offline-ai-tools/)  
5. On-Device LLMs in 2026: What Changed, What Matters, What's Next, accessed on February 22, 2026, [https://www.edge-ai-vision.com/2026/01/on-device-llms-in-2026-what-changed-what-matters-whats-next/](https://www.edge-ai-vision.com/2026/01/on-device-llms-in-2026-what-changed-what-matters-whats-next/)  
6. Unleashing OpenClaw: The Ultimate Guide to Local AI Agents for Developers in 2026 \- DEV Community, accessed on February 22, 2026, [https://dev.to/mechcloud\_academy/unleashing-openclaw-the-ultimate-guide-to-local-ai-agents-for-developers-in-2026-3k0h](https://dev.to/mechcloud_academy/unleashing-openclaw-the-ultimate-guide-to-local-ai-agents-for-developers-in-2026-3k0h)  
7. Small Language Models are the Future of Agentic AI \- Research at NVIDIA, accessed on February 22, 2026, [https://research.nvidia.com/labs/lpr/slm-agents/](https://research.nvidia.com/labs/lpr/slm-agents/)  
8. SLM vs LLM: Accuracy, Latency, Cost Trade-Offs 2026 | Label Your Data, accessed on February 22, 2026, [https://labelyourdata.com/articles/llm-fine-tuning/slm-vs-llm](https://labelyourdata.com/articles/llm-fine-tuning/slm-vs-llm)  
9. Xiaomi MiMo-V2-Flash a Technical Review | by Barnacle Goose \- Medium, accessed on February 22, 2026, [https://medium.com/@leucopsis/xiaomi-mimo-v2-flash-a-technical-review-6a69e77beecc](https://medium.com/@leucopsis/xiaomi-mimo-v2-flash-a-technical-review-6a69e77beecc)  
10. (PDF) GraphPilot: GUI Task Automation with One-Step LLM Reasoning Powered by Knowledge Graph \- ResearchGate, accessed on February 22, 2026, [https://www.researchgate.net/publication/400500469\_GraphPilot\_GUI\_Task\_Automation\_with\_One-Step\_LLM\_Reasoning\_Powered\_by\_Knowledge\_Graph](https://www.researchgate.net/publication/400500469_GraphPilot_GUI_Task_Automation_with_One-Step_LLM_Reasoning_Powered_by_Knowledge_Graph)  
11. Top 10 Open-source Reasoning Models in 2026 \- Clarifai, accessed on February 22, 2026, [https://www.clarifai.com/blog/top-10-open-source-reasoning-models-in-2026](https://www.clarifai.com/blog/top-10-open-source-reasoning-models-in-2026)  
12. How Much RAM Does AI Need? 8GB vs 16GB vs 32GB (2025 Guide) | Local AI Master, accessed on February 22, 2026, [https://localaimaster.com/blog/ram-requirements-local-ai](https://localaimaster.com/blog/ram-requirements-local-ai)  
13. Evolution of Meta's LLaMA Models and Parameter-Efficient Fine-Tuning of Large Language Models: A Survey \- arXiv, accessed on February 22, 2026, [https://arxiv.org/html/2510.12178v1](https://arxiv.org/html/2510.12178v1)  
14. Top 7 Small Language Models You Can Run on a Laptop \- MachineLearningMastery.com, accessed on February 22, 2026, [https://machinelearningmastery.com/top-7-small-language-models-you-can-run-on-a-laptop/](https://machinelearningmastery.com/top-7-small-language-models-you-can-run-on-a-laptop/)  
15. Accelerate Large-Scale LLM Inference and KV Cache Offload with CPU-GPU Memory Sharing | NVIDIA Technical Blog, accessed on February 22, 2026, [https://developer.nvidia.com/blog/accelerate-large-scale-llm-inference-and-kv-cache-offload-with-cpu-gpu-memory-sharing/](https://developer.nvidia.com/blog/accelerate-large-scale-llm-inference-and-kv-cache-offload-with-cpu-gpu-memory-sharing/)  
16. Analysis of Llama 4's 10 Million Token Context Window Claim | by Sander Ali Khowaja, accessed on February 22, 2026, [https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde](https://sandar-ali.medium.com/analysis-of-llama-4s-10-million-token-context-window-claim-9e68ee5abcde)  
17. On-Device LLMs: State of the Union, 2026 \- Vikas Chandra, accessed on February 22, 2026, [https://v-chandra.github.io/on-device-llms/](https://v-chandra.github.io/on-device-llms/)  
18. The 8-Series Reimagined: Snapdragon 8 Gen 5 Delivers Premium Performance and Experiences | Qualcomm, accessed on February 22, 2026, [https://www.qualcomm.com/news/releases/2025/11/-the-8-series-reimagined--snapdragon-8-gen-5-delivers-premium-pe](https://www.qualcomm.com/news/releases/2025/11/-the-8-series-reimagined--snapdragon-8-gen-5-delivers-premium-pe)  
19. microsoft/bitnet-b1.58-2B-4T \- Hugging Face, accessed on February 22, 2026, [https://huggingface.co/microsoft/bitnet-b1.58-2B-4T](https://huggingface.co/microsoft/bitnet-b1.58-2B-4T)  
20. microsoft/BitNet: Official inference framework for 1-bit LLMs \- GitHub, accessed on February 22, 2026, [https://github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)  
21. The Future of AI Efficiency with BitNet b1.58 and 1-Bit LLMs \- CloudThat, accessed on February 22, 2026, [https://www.cloudthat.com/resources/blog/the-future-of-ai-efficiency-with-bitnet-b1-58-and-1-bit-llms](https://www.cloudthat.com/resources/blog/the-future-of-ai-efficiency-with-bitnet-b1-58-and-1-bit-llms)  
22. Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models | OpenReview, accessed on February 22, 2026, [https://openreview.net/forum?id=Urt7MPg1u0](https://openreview.net/forum?id=Urt7MPg1u0)  
23. I benchmarked 1 bit models on CPU and the results surprised me : r/LocalLLaMA \- Reddit, accessed on February 22, 2026, [https://www.reddit.com/r/LocalLLaMA/comments/1r2ez9c/i\_benchmarked\_1\_bit\_models\_on\_cpu\_and\_the\_results/](https://www.reddit.com/r/LocalLLaMA/comments/1r2ez9c/i_benchmarked_1_bit_models_on_cpu_and_the_results/)  
24. Cross-platform mobile development with C++ | Microsoft Learn, accessed on February 22, 2026, [https://learn.microsoft.com/en-us/cpp/cross-platform/visual-cpp-for-cross-platform-mobile-development?view=msvc-170](https://learn.microsoft.com/en-us/cpp/cross-platform/visual-cpp-for-cross-platform-mobile-development?view=msvc-170)  
25. Create an Android Native Activity App | Microsoft Learn, accessed on February 22, 2026, [https://learn.microsoft.com/en-us/cpp/cross-platform/create-an-android-native-activity-app?view=msvc-170](https://learn.microsoft.com/en-us/cpp/cross-platform/create-an-android-native-activity-app?view=msvc-170)  
26. An Introduction to Speculative Decoding for Reducing Latency in AI Inference \- NVidia, accessed on February 22, 2026, [https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/](https://developer.nvidia.com/blog/an-introduction-to-speculative-decoding-for-reducing-latency-in-ai-inference/)  
27. LFM2.5-1.2B-Thinking: On-Device Reasoning Under 1GB | Liquid AI, accessed on February 22, 2026, [https://www.liquid.ai/blog/lfm2-5-1-2b-thinking-on-device-reasoning-under-1gb](https://www.liquid.ai/blog/lfm2-5-1-2b-thinking-on-device-reasoning-under-1gb)  
28. Liquid AI's Small Reasoning Model Mixes Attention With Convolutional Layers for Efficiency \- DeepLearning.AI, accessed on February 22, 2026, [https://www.deeplearning.ai/the-batch/liquid-ais-small-reasoning-model-mixes-attention-with-convolutional-layers-for-efficiency/](https://www.deeplearning.ai/the-batch/liquid-ais-small-reasoning-model-mixes-attention-with-convolutional-layers-for-efficiency/)  
29. LiquidAI/LFM2.5-1.2B-Thinking \- Hugging Face, accessed on February 22, 2026, [https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)  
30. Liquid AI Releases LFM2.5-1.2B-Thinking: a 1.2B Parameter Reasoning Model That Fits Under 1 GB On-Device \- MarkTechPost, accessed on February 22, 2026, [https://www.marktechpost.com/2026/01/20/liquid-ai-releases-lfm2-5-1-2b-thinking-a-1-2b-parameter-reasoning-model-that-fits-under-1-gb-on-device/](https://www.marktechpost.com/2026/01/20/liquid-ai-releases-lfm2-5-1-2b-thinking-a-1-2b-parameter-reasoning-model-that-fits-under-1-gb-on-device/)  
31. Reasoning reimagined: Introducing Phi-4-mini-flash-reasoning | Microsoft Azure Blog, accessed on February 22, 2026, [https://azure.microsoft.com/en-us/blog/reasoning-reimagined-introducing-phi-4-mini-flash-reasoning/](https://azure.microsoft.com/en-us/blog/reasoning-reimagined-introducing-phi-4-mini-flash-reasoning/)  
32. phi-4-mini-flash-reasoning Model by Microsoft \- NVIDIA NIM APIs, accessed on February 22, 2026, [https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning/modelcard](https://build.nvidia.com/microsoft/phi-4-mini-flash-reasoning/modelcard)  
33. One year of Phi: Small language models making big leaps in AI | Microsoft Azure Blog, accessed on February 22, 2026, [https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/](https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/)  
34. Papers Explained 358: Phi-4-Reasoning | by Ritvik Rastogi \- Medium, accessed on February 22, 2026, [https://ritvik19.medium.com/papers-explained-358-phi-4-reasoning-98c1d3b5e52d](https://ritvik19.medium.com/papers-explained-358-phi-4-reasoning-98c1d3b5e52d)  
35. Ultimate Guide \- The Best Small LLMs Under 10B Parameters in 2026 \- SiliconFlow, accessed on February 22, 2026, [https://www.siliconflow.com/articles/en/best-small-LLMs-under-10B-parameters](https://www.siliconflow.com/articles/en/best-small-LLMs-under-10B-parameters)  
36. Small Language Models are the Future of Agentic AI \- arXiv, accessed on February 22, 2026, [https://arxiv.org/pdf/2506.02153](https://arxiv.org/pdf/2506.02153)  
37. OmniParser for Pure Vision Based GUI Agent, accessed on February 22, 2026, [https://microsoft.github.io/OmniParser/](https://microsoft.github.io/OmniParser/)  
38. OmniParser for pure vision-based GUI agent \- Microsoft Research, accessed on February 22, 2026, [https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/](https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/)  
39. Ultimate Guide \- The Best Lightweight LLMs for Mobile Devices in 2026 \- SiliconFlow, accessed on February 22, 2026, [https://www.siliconflow.com/articles/en/best-lightweight-LLMs-for-mobile-devices](https://www.siliconflow.com/articles/en/best-lightweight-LLMs-for-mobile-devices)  
40. Alibaba Cloud Model Studio:Model list, accessed on February 22, 2026, [https://www.alibabacloud.com/help/en/model-studio/models](https://www.alibabacloud.com/help/en/model-studio/models)  
41. Gemma 3 model overview | Google AI for Developers, accessed on February 22, 2026, [https://ai.google.dev/gemma/docs/core](https://ai.google.dev/gemma/docs/core)  
42. Lightweight, Multimodal, Multilingual Gemma 3 Models Are Streamlined for Performance, accessed on February 22, 2026, [https://developer.nvidia.com/blog/lightweight-multimodal-multilingual-gemma-3-models-are-streamlined-for-performance/](https://developer.nvidia.com/blog/lightweight-multimodal-multilingual-gemma-3-models-are-streamlined-for-performance/)  
43. Introducing Gemma 3: The most capable model you can run on a single GPU or TPU, accessed on February 22, 2026, [https://blog.google/innovation-and-ai/technology/developers-tools/gemma-3/](https://blog.google/innovation-and-ai/technology/developers-tools/gemma-3/)  
44. OmniParser V2: Turning Any LLM into a Computer Use Agent \- Microsoft Research, accessed on February 22, 2026, [https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/](https://www.microsoft.com/en-us/research/articles/omniparser-v2-turning-any-llm-into-a-computer-use-agent/)  
45. OmniParser: Microsoft's Breakthrough in AI-Powered UI Interaction | by Malyaj Mishra | Data Science in Your Pocket | Medium, accessed on February 22, 2026, [https://medium.com/data-science-in-your-pocket/omniparser-microsofts-breakthrough-in-ai-powered-ui-interaction-08c7c2cc28d5](https://medium.com/data-science-in-your-pocket/omniparser-microsofts-breakthrough-in-ai-powered-ui-interaction-08c7c2cc28d5)  
46. How to Run Microsoft's OmniParser V2 Locally? \- Analytics Vidhya, accessed on February 22, 2026, [https://www.analyticsvidhya.com/blog/2025/02/run-omniparser-v2-locally/](https://www.analyticsvidhya.com/blog/2025/02/run-omniparser-v2-locally/)  
47. microsoft/OmniParser-v2.0 \- Hugging Face, accessed on February 22, 2026, [https://huggingface.co/microsoft/OmniParser-v2.0](https://huggingface.co/microsoft/OmniParser-v2.0)  
48. 1 Introduction \- arXiv, accessed on February 22, 2026, [https://arxiv.org/html/2601.17418v1](https://arxiv.org/html/2601.17418v1)  
49. \[2510.15312\] Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated Execution \- arXiv, accessed on February 22, 2026, [https://arxiv.org/abs/2510.15312](https://arxiv.org/abs/2510.15312)  
50. Scaling LLM Test-Time Compute with Mobile NPU on Smartphones \- arXiv.org, accessed on February 22, 2026, [https://arxiv.org/html/2509.23324v1](https://arxiv.org/html/2509.23324v1)  
51. Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated Execution \- arXiv.org, accessed on February 22, 2026, [https://arxiv.org/html/2510.15312v4](https://arxiv.org/html/2510.15312v4)  
52. Accelerating Mobile Language Model via Speculative Decoding and NPU-Coordinated Execution \- arXiv, accessed on February 22, 2026, [https://arxiv.org/html/2510.15312v3](https://arxiv.org/html/2510.15312v3)  
53. NeurIPS Poster ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning, accessed on February 22, 2026, [https://neurips.cc/virtual/2025/poster/119250](https://neurips.cc/virtual/2025/poster/119250)  
54. ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning, accessed on February 22, 2026, [https://openreview.net/forum?id=D1PeGJtVEu](https://openreview.net/forum?id=D1PeGJtVEu)  
55. ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning, accessed on February 22, 2026, [https://arxiv.org/html/2507.02834v3](https://arxiv.org/html/2507.02834v3)  
56. pytorch/executorch: On-device AI across mobile, embedded and edge for PyTorch \- GitHub, accessed on February 22, 2026, [https://github.com/pytorch/executorch](https://github.com/pytorch/executorch)  
57. My 5-Step Framework for Building Local AI Agents (No API Keys Needed) | by Shivam Rathod | Medium, accessed on February 22, 2026, [https://medium.com/@shivamr021/%EF%B8%8F-my-5-step-framework-for-building-local-ai-agents-no-api-keys-needed-030aa6e12424](https://medium.com/@shivamr021/%EF%B8%8F-my-5-step-framework-for-building-local-ai-agents-no-api-keys-needed-030aa6e12424)  
58. Small Language Models for Agentic AI \- Sundeep Teki, accessed on February 22, 2026, [https://www.sundeepteki.org/blog/small-language-models-for-agentic-ai](https://www.sundeepteki.org/blog/small-language-models-for-agentic-ai)

[image1]: <data:image/png;base64,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>

[image2]: <data:image/png;base64,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>

[image3]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFEAAAAWCAYAAAC40nDiAAACfUlEQVR4Xu2Yz0sUYRjHn0UDpYiiSIMwigiCwEA6dLC9VCCiF4Xq0jXo2qG8+CeUdOkQEdJBvHWIkOig0CFI8CQI4kERgkCi0KCgH98vz7w087iv+87OTO6w+4EPLO8zOzvvd95533dWpE2bZucEPAm7bKGFYP+P2MYQDsD78AuchVeT5ZbiFfwJP8PbpualAqfgH9htanly2DakpEN0lPBp4TVngd8/K/7+HoQvRTMJogeuwU1byAlODbyj67aQgipchUuRy/By4ohwzsEX8CscMLU4PP930UDrwrvLDmbppA+OGo6gB5Lirhpm4CfRkeO4ALeiWiicsjhgXH93ZO8QWeMxh2yhFkWG6MgSIsP6CI/G2o6LjkaGm5aWDJHfm5dkZ/iZbY2cs9AQOS8WRRlD7IffJDDEm/A3fGoLOVLGEPkbb+GgLcThqsmtDfdET6T2KsRFwW3AQ/Rt0ssYIumF2/Ah7BNdnBIMiQbI/dApU3OwfQFuBDqsX9tFWUOswA+iOb2B55Plf1wUfVPhwUWRJUTOS1yJuSI73N6WK3daQkM8I7p35m/VxZ10P1ZnPh6TojewamqOOfHvE1lz3ICLUuORM4SG2NDqXFSInCcfi4ZoL8itgKz5noRR+AtOiD5elJ/Zdj06hvP5O9HzXInafFwSfS/m28g1U4vTNCGyU7Wcjuq8QL51cHfg22IxtHuinX4Wyc93oprjrujcNRJri+P6aa/FNyKbJsRQOC+7YLPAPxT4qOdBqULkaHoEb9lCA/Dx7rSNDZIqRLdd+GHa/xdj8LUEXuwenIbvbWMG3EtIfMqoyzHRP2Q5N3Eyb1VW4HM4Lv7/G9vkwV8kubflSWQVrgAAAABJRU5ErkJggg==>

[image4]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAZCAYAAADOtSsxAAAD30lEQVR4Xu2YS6iNURTHlzwib4lE3AyU10BC5DUwIJHXQB4lBgyMKGQgA4rcIjEgkgEiiaSQuqcUMb4YKURKoRTlbf3ss+/dZ32P851H3zm3zq/+nb61v2+/1t5rr31EWrRo0TMZpupvjU3IUFVfa6yEQaoxEq3EPufNLXGDa3aWqx5YYxYOqd6p/qo+qb6p9qgGqCaoJne/mjtrVG3B8yTVWaMNQTlsVF0vlqF21YiSN8rDrjsi0bbitK34DWwSN2eZYIKPql6rtkj3KhuiuqS6p3qpGl605w0DeWFsI1UHVH/ELZiLEh3wdNUxce8wtnmqXiVvlGeguBW9RVw7j1TrVeuKwslnVB9Uh90n/yFa4JTUqEGHO8VVzEpPYr+4dxrBNNVn1U5bIM4Jz8T1ba8pg1Gqx6oFtqAKlqg+SnIUIGyvMLa7RbHAY7ktrvPXJN1TbH8abwRMbNLAGXRBuneA5URRla76OFjdBXFtemaI2yGAfVZQBttV31Vzjb0LOs7qYpWlgQMK1pgDfoIfqgaXFnXBxDOOO7ZAeaoaZ41VwCRzqJ40dla3r59wbUMgDuEcjdudMlpcx6+o+pgyyxzVQmvMgYmq96pztiCAlck4cFII59hKY6sW34+wPnYVESQtLSaT5OwhEYjMMd6kUiqvB8RjfzBlVTmIqUwuB10S/p0vgW226knwXCvMFW34w5f+3FDND1+KAeewM3ECzogUJG1tUi8+sMobP7n2cAuZqfqq+ll8JmyRtZEG1oPwnAmh3bHBc9JOIERGHOArReGhAv1U51VvxDWKfojLNvKGFVfOAT48+Anarboq6UlFJXD4kwSQyoaQFfmwwu/loCwk1gG+oCBRB4T4hmmsEWTZAZxl3FF4j6zkuWpqyRu1Qbih7lfGHkIS02GN4s4J7lGxDiBF4nLDAJKg4beSLZPASX7HZFU5vAN22YKAMETwV8WOktLaYAIviKubgzQOdhrvxN1TfN9YILHzTApKeNmq6h3YuTjsE9dwbAqVE351x2YRAT4TYgdkZZm4b9KywKXi8vhfqkWBnbnikndfXB2nJP6u4cOjTV+7mCIuV6YSnMHV+aa4jIKPjkvKJSIHmBgmiPOHLCsJdkiWHRVCSCC8dkg0DPu8P9ytSSIB4ECOg7T1tzhHJoLn2lSrxB16iyX5RG8E9IlBpJ1D41WrrTEDrFAcbB1QL1jE5cJ808NW7xS3zesNK7PdGusIk3/QGnsimf5ZrBB2/mlJjv+10qZaa409Gf5a4B/buMOuGqgn8V/KGqGvXAbr1demgaSgkkynUbBQNltjixYtWjQB/wCBo9tRwlw9cgAAAABJRU5ErkJggg==>

[image5]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAYCAYAAADzoH0MAAAA30lEQVR4Xu2SMQtBURiGX8VmkSKbXSb5B5QSv8HGTJIfYLcpMZik/AeyyWQyGfwFxYj365zDvZ9u16z71DOc7z3nO7fzXSDC0adTZca3A1jSuc3Euj8GRvRJDzSlMiFJN7RFY/7I0IZpcKE5lQkduqIJHTgaMA1utKQy4UgLuuhFDsnhB62oTG7tqdoXWXqG+YqBp56ne886EHmkLUyDma3JzfLiQ7sOZQHTYG3XTbqj6feOELr4TGJCxwgYWRBuEleY/0H/TKGU6R2mSVVlP+EmcdLBr8RpjRZ1EPHXvABsTSiYRlzjcQAAAABJRU5ErkJggg==>

[image6]: <data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAbCAYAAACjkdXHAAAA9ElEQVR4Xu2RPw4BQRSHn0QpESEOoCNEoZI4AIVOonAAF5C4BxWRKEkUopE4A6VSo9FpJAqF8PvlzdgxZDnAfsmXnX1/dt7MikQ04PhPWftGCbbgCj7g0bxbO3Bkcn3T88FAtGDqJwwn2PSDJAW3os1dL2dZw5ofJHl4hldYceIFmDHrJSw7uRc8F3fl7pyCJOBC9MNkDrNm/cIdeSd6q3vzznsIxY58gz3RGx7Cu+hEoXwbOQ03EoxMeIwPJqLNfFpycCZBQxzWg3SAPW/YiEWY9IPkYvz6G0R3dad6wz+vSwy24cENVkVvl42/ZB3rIyL+4wm53T9I1ue+FgAAAABJRU5ErkJggg==>